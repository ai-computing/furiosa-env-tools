#!/usr/bin/env python3
"""
FuriosaAI NPUìš© Llama-3.1-8B-Instruct ëª¨ë¸ ì»´íŒŒì¼

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” furiosa_llm.artifact.builderë¥¼ ì‚¬ìš©í•˜ì—¬
Llama ëª¨ë¸ì„ FuriosaAI WARBOY NPUì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ì»´íŒŒì¼í•©ë‹ˆë‹¤.
"""

from furiosa_llm.artifact.builder import ArtifactBuilder
from pathlib import Path
import sys
import os

# ì˜¤í”„ë¼ì¸ ëª¨ë“œ í™œì„±í™” (Hugging Face Hub ì ‘ê·¼ ì°¨ë‹¨)
os.environ['HF_HUB_OFFLINE'] = '1'
os.environ['TRANSFORMERS_OFFLINE'] = '1'

# ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
OUTPUT_DIR = Path("./Output-Llama-3.1-8B-Instruct")

print("=" * 70)
print("ðŸš€ FuriosaAI Llama-3.1-8B-Instruct Compilation")
print("=" * 70)

# Prefill ë²„í‚·: (batch_size, sequence_length)
# ë‹¤ì–‘í•œ ìž…ë ¥ í¬ê¸°ì— ìµœì í™”ëœ ëª¨ë¸ ìƒì„±
RELEASE_PREFILL_BUCKETS = [
    (1, 256), (1, 320), (1, 384), (1, 512), (1, 640),
    (1, 768), (1, 1024), (2, 1024), (4, 1024),
]

# Decode ë²„í‚·: (batch_size, kv_cache_length)
# ë””ì½”ë”© ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•  KV ìºì‹œ í¬ê¸° ì„¤ì •
RELEASE_DECODE_BUCKETS = [
    # 1K context
    *[(1, 1024),  (4, 1024),   (8, 1024), (16, 1024), (32, 1024), (64, 1024)],
    # 2K context
    *[(1, 2048),  (4, 2048),   (8, 2048), (16, 2048), (32, 2048)],
    # 4K context
    *[(1, 4096),  (4, 4096),   (8, 4096), (16, 4096), (32, 4096)],
    # 8K context
    *[(1, 8192),  (4, 8192),   (8, 8192), (16, 8192)],
    # 16K context
    *[(1, 16384), (4, 16384), (8, 16384)],
    # 32K context
    *[(1, 32768), (4, 32768)],
]

print("\nðŸ“‹ Compilation Configuration:")
print(f"   Model: meta-llama/Llama-3.1-8B-Instruct")
print(f"   Tensor Parallel Size: 8")
print(f"   Max Sequence Length: 32,768 tokens")
print(f"   Prefill Chunk Size: 8,192 tokens")
print(f"   Output Directory: {OUTPUT_DIR.absolute()}")
print(f"   Prefill Buckets: {len(RELEASE_PREFILL_BUCKETS)} configurations")
print(f"   Decode Buckets: {len(RELEASE_DECODE_BUCKETS)} configurations")

print("\nðŸ” Prefill Buckets (batch, seq_len):")
for i, (bs, sl) in enumerate(RELEASE_PREFILL_BUCKETS, 1):
    print(f"   {i}. Batch={bs}, SeqLen={sl}")

print("\nðŸ” Decode Buckets (first 10):")
for i, (bs, kv) in enumerate(RELEASE_DECODE_BUCKETS[:10], 1):
    print(f"   {i}. Batch={bs}, KV_Cache={kv}")
print(f"   ... and {len(RELEASE_DECODE_BUCKETS) - 10} more configurations")

print("\n" + "=" * 70)
print("âš™ï¸  Initializing ArtifactBuilder...")
print("=" * 70)

try:
    # ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ í™•ì¸ (ì›ë³¸ HF ëª¨ë¸)
    LOCAL_MODEL_PATH = Path("./models/Llama-3.1-8B-Instruct-original").resolve()

    # ë°±ì—…ëœ ì»´íŒŒì¼ ëª¨ë¸ ê²½ë¡œë„ í™•ì¸
    COMPILED_BACKUP = Path("./models/Llama-3.1-8B-Instruct-compiled-backup").resolve()

    if LOCAL_MODEL_PATH.exists():
        print(f"   Using local model: {LOCAL_MODEL_PATH}")
        print(f"   Offline mode: Enabled (no Hugging Face connection)")
        model_path = str(LOCAL_MODEL_PATH)
    else:
        print(f"   âŒ Error: Original model not found at {LOCAL_MODEL_PATH}")
        print("   Please download the model first:")
        print("   $ python3 download_model.py")
        print()
        if COMPILED_BACKUP.exists():
            print(f"   â„¹ï¸  Note: Pre-compiled model is backed up at {COMPILED_BACKUP}")
            print("      If you want to use the pre-compiled model without recompiling,")
            print("      you can use it directly for inference.")
        sys.exit(1)

    # ArtifactBuilder ì´ˆê¸°í™”
    builder = ArtifactBuilder(
        model_id_or_path=model_path,
        artifact_name="Llama-3.1-8B-Instruct-FuriosaAI",
        tensor_parallel_size=8,              # 8ê°œ NPUë¡œ ë³‘ë ¬ ì²˜ë¦¬
        prefill_buckets=RELEASE_PREFILL_BUCKETS,
        decode_buckets=RELEASE_DECODE_BUCKETS,
        max_seq_len_to_capture=32 * 1024,    # ìµœëŒ€ 32K í† í°
        prefill_chunk_size=8 * 1024,          # 8K í† í° ì²­í¬
    )

    print("\nâœ… ArtifactBuilder initialized successfully!")
    print("\n" + "=" * 70)
    print("ðŸ”¨ Starting compilation process...")
    print("=" * 70)
    print("\nâš ï¸  This may take a significant amount of time (hours).")
    print("âš ï¸  Progress will be displayed below.\n")

    # ì»´íŒŒì¼ ì‹¤í–‰
    builder.build(
        str(OUTPUT_DIR),
        num_pipeline_builder_workers=8,      # 8ê°œ ì›Œì»¤ë¡œ ë³‘ë ¬ ì»´íŒŒì¼
    )

    print("\n" + "=" * 70)
    print("âœ… Compilation completed successfully!")
    print("=" * 70)
    print(f"\nðŸ“ Compiled artifacts saved to: {OUTPUT_DIR.absolute()}")

    # ì¶œë ¥ íŒŒì¼ í™•ì¸
    if OUTPUT_DIR.exists():
        print("\nðŸ“‹ Generated files:")
        for file in sorted(OUTPUT_DIR.rglob("*")):
            if file.is_file():
                size_mb = file.stat().st_size / (1024 * 1024)
                print(f"   - {file.relative_to(OUTPUT_DIR)}: {size_mb:.2f} MB")

    print("\nðŸŽ‰ Model is ready for inference on FuriosaAI NPU!")

except ImportError as e:
    print("\nâŒ Error: furiosa_llm package not found!")
    print("\nðŸ“¦ Please install furiosa-llm:")
    print("   $ pip install furiosa-llm")
    print("\nðŸ“š Documentation: https://developer.furiosa.ai/")
    sys.exit(1)

except Exception as e:
    print(f"\nâŒ Compilation failed with error:")
    print(f"   {type(e).__name__}: {e}")
    print("\nðŸ” Troubleshooting tips:")
    print("   1. Ensure FuriosaAI NPU hardware is available")
    print("   2. Check that all drivers are properly installed")
    print("   3. Verify sufficient disk space (>50GB recommended)")
    print("   4. Check system memory (>64GB recommended)")
    print("\nðŸ“š For more help, visit: https://developer.furiosa.ai/")
    sys.exit(1)
