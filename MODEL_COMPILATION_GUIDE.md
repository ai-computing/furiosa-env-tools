# FuriosaAI NPUë¥¼ ìœ„í•œ Llama-3.1-8B-Instruct ì»´íŒŒì¼ ê°€ì´ë“œ

ì´ ê°€ì´ë“œëŠ” Llama-3.1-8B-Instruct ëª¨ë¸ì„ FuriosaAI NPUì—ì„œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì¤€ë¹„ì™€ ì»´íŒŒì¼ ê³¼ì •ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

1. [í™˜ê²½ ì„¤ì •](#í™˜ê²½-ì„¤ì •)
2. [ëª¨ë¸ ë‹¤ìš´ë¡œë“œ](#ëª¨ë¸-ë‹¤ìš´ë¡œë“œ)
3. [ì»´íŒŒì¼ ì¤€ë¹„](#ì»´íŒŒì¼-ì¤€ë¹„)
4. [ëª¨ë¸ ì»´íŒŒì¼](#ëª¨ë¸-ì»´íŒŒì¼)
5. [ì¶”ê°€ ë¦¬ì†ŒìŠ¤](#ì¶”ê°€-ë¦¬ì†ŒìŠ¤)

## ğŸ”§ í™˜ê²½ ì„¤ì •

### í•„ìˆ˜ ìš”êµ¬ì‚¬í•­

- **OS**: Ubuntu 22.04 LTS (Jammy) ì´ìƒ ë˜ëŠ” Debian Bookworm ì´ìƒ
- **Kernel**: Linux 6.3+
- **Python**: 3.8+
- **FuriosaAI SDK**: 2025.3.1 ì´ìƒ

### ì„¤ì¹˜ëœ íŒ¨í‚¤ì§€

ì´ë¯¸ ì„¤ì¹˜ëœ íŒ¨í‚¤ì§€ë“¤:
```bash
âœ… FuriosaAI Driver (furiosa-driver-rngd 2025.3.1-4)
âœ… FuriosaAI Runtime (furiosa-pert-rngd 2025.3.1-4)
âœ… FuriosaAI SMI (furiosa-smi 2025.3.0-4)
âœ… Transformers 4.57.1
âœ… PyTorch 2.9.0
âœ… Hugging Face Hub 0.36.0
```

## ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

### ìë™ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™”
export PATH="/root/.local/bin:$PATH"
source .venv/bin/activate

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
python download_model.py
```

ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ ìœ„ì¹˜: `./models/Llama-3.1-8B-Instruct/`

### ëª¨ë¸ ì •ë³´

- **Model ID**: `furiosa-ai/Llama-3.1-8B-Instruct`
- **Size**: ~15GB
- **Architecture**: Llama 3.1
- **Parameters**: 8B
- **License**: Llama 3 Community License

## ğŸ› ï¸ ì»´íŒŒì¼ ì¤€ë¹„

### 1. ëª¨ë¸ êµ¬ì„± í™•ì¸

```bash
python prepare_compilation.py
```

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒì„ í™•ì¸í•©ë‹ˆë‹¤:
- âœ… ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸
- âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ
- âœ… ëª¨ë¸ configuration ê²€ì¦
- âœ… í…ŒìŠ¤íŠ¸ ì…ë ¥ ìƒì„±

### 2. ì»´íŒŒì¼ ì„¤ì •

`compile_for_furiosa.py`ì—ì„œ ë‹¤ìŒ ì„¤ì •ì„ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
compile_config = {
    "batch_size": 1,
    "max_seq_length": 2048,
    "precision": "fp16",  # or "int8" for quantization
    "target": "warboy",   # FuriosaAI NPU target
}
```

## ğŸš€ ëª¨ë¸ ì»´íŒŒì¼

### FuriosaAI LLM SDK ì‚¬ìš©

#### ì˜µì…˜ A: CLI ë°©ì‹

```bash
furiosa-llm compile \
  --model-path ./models/Llama-3.1-8B-Instruct \
  --output-path ./compiled_models/llama-3.1-8b-furiosa \
  --batch-size 1 \
  --max-seq-length 2048 \
  --target warboy
```

#### ì˜µì…˜ B: Python API ë°©ì‹

```python
from furiosa_llm import compile_model

compiled_model = compile_model(
    model_path='./models/Llama-3.1-8B-Instruct',
    output_path='./compiled_models/llama-3.1-8b-furiosa',
    batch_size=1,
    max_seq_length=2048,
    target='warboy'
)
```

### ì»´íŒŒì¼ ì˜µì…˜ ì„¤ëª…

| ì˜µì…˜ | ì„¤ëª… | ê¸°ë³¸ê°’ |
|------|------|--------|
| `batch_size` | ë°°ì¹˜ í¬ê¸° | 1 |
| `max_seq_length` | ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ | 2048 |
| `precision` | ì •ë°€ë„ (fp16/int8) | fp16 |
| `blockwise_compile` | ë¸”ë¡ë³„ ì»´íŒŒì¼ | True |
| `kv_cache` | KV ìºì‹œ í™œì„±í™” | True |

### ì–‘ìí™” (Quantization)

INT8 ì–‘ìí™”ë¡œ ëª¨ë¸ í¬ê¸°ì™€ ì¶”ë¡  ì†ë„ ê°œì„ :

```python
compile_config = {
    "precision": "int8",
    "quantization": {
        "enabled": True,
        "method": "dynamic",  # "static" ë˜ëŠ” "dynamic"
    }
}
```

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
furiosa-env-tools/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ Llama-3.1-8B-Instruct/    # ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ tokenizer.json
â”‚       â”œâ”€â”€ model-*.safetensors
â”‚       â””â”€â”€ ...
â”œâ”€â”€ compiled_models/               # ì»´íŒŒì¼ëœ ëª¨ë¸
â”‚   â””â”€â”€ llama-3.1-8b-furiosa/
â”‚       â””â”€â”€ compile_config.json
â”œâ”€â”€ download_model.py              # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ prepare_compilation.py         # ì»´íŒŒì¼ ì¤€ë¹„ ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ compile_for_furiosa.py        # ì»´íŒŒì¼ í…œí”Œë¦¿ ìŠ¤í¬ë¦½íŠ¸
```

## ğŸ§ª í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

### 1. ì»´íŒŒì¼ í™•ì¸

```bash
# ì»´íŒŒì¼ëœ ëª¨ë¸ íŒŒì¼ í™•ì¸
ls -lh compiled_models/llama-3.1-8b-furiosa/

# FuriosaAI SMIë¡œ NPU ìƒíƒœ í™•ì¸
furiosa-smi
```

### 2. ì¶”ë¡  í…ŒìŠ¤íŠ¸

```python
from transformers import AutoTokenizer

# í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("./models/Llama-3.1-8B-Instruct")

# í…ŒìŠ¤íŠ¸ ì…ë ¥
text = "Hello, I am a language model"
inputs = tokenizer(text, return_tensors="pt")

# FuriosaAI ëª¨ë¸ë¡œ ì¶”ë¡ 
# (ì‹¤ì œ ì½”ë“œëŠ” furiosa-llm API ì‚¬ìš©)
```

## ğŸ“š ì¶”ê°€ ë¦¬ì†ŒìŠ¤

### ê³µì‹ ë¬¸ì„œ

- [FuriosaAI Developer Center](https://developer.furiosa.ai/)
- [FuriosaAI LLM Documentation](https://developer.furiosa.ai/latest/en/)
- [Optimum FuriosaAI](https://github.com/huggingface/optimum-furiosa)

### Hugging Face

- [Model Card](https://huggingface.co/furiosa-ai/Llama-3.1-8B-Instruct)
- [FuriosaAI Hub](https://huggingface.co/furiosa-ai)

### ì°¸ê³ ì‚¬í•­

- ğŸ”´ **FuriosaAI NPU í•˜ë“œì›¨ì–´**: ì‹¤ì œ ì»´íŒŒì¼ ë° ì¶”ë¡ ì„ ìœ„í•´ì„œëŠ” FuriosaAI WARBOY NPUê°€ í•„ìš”í•©ë‹ˆë‹¤
- ğŸ“¦ **furiosa-llm íŒ¨í‚¤ì§€**: ëª¨ë¸ ì»´íŒŒì¼ì„ ìœ„í•´ `furiosa-llm` íŒ¨í‚¤ì§€ ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤
- ğŸ’¾ **ë””ìŠ¤í¬ ê³µê°„**: ì›ë³¸ ëª¨ë¸(~15GB) + ì»´íŒŒì¼ëœ ëª¨ë¸(~10-20GB) ê³µê°„ í•„ìš”

## âš ï¸ ë¬¸ì œ í•´ê²°

### WSL2 í™˜ê²½

í˜„ì¬ WSL2 í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. WSL2ì—ì„œëŠ”:
- âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì¤€ë¹„ ê°€ëŠ¥
- âœ… ì»´íŒŒì¼ ìŠ¤í¬ë¦½íŠ¸ ì¤€ë¹„ ê°€ëŠ¥
- âš ï¸ ì‹¤ì œ NPU í•˜ë“œì›¨ì–´ ì ‘ê·¼ ë¶ˆê°€ëŠ¥
- âš ï¸ ì‹¤ì œ ì»´íŒŒì¼ ë° ì¶”ë¡ ì€ ì‹¤ì œ í•˜ë“œì›¨ì–´ì—ì„œ ìˆ˜í–‰ í•„ìš”

### ë””ë°”ì´ìŠ¤ ë¯¸ê²€ì¶œ

```bash
# NPU ì¥ì¹˜ í™•ì¸
furiosa-smi

# ì¥ì¹˜ê°€ ì—†ëŠ” ê²½ìš°
# - í•˜ë“œì›¨ì–´ ì—°ê²° í™•ì¸
# - ë“œë¼ì´ë²„ ì„¤ì¹˜ í™•ì¸
# - BIOS ì„¤ì • í™•ì¸
```

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

1. âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ
2. âœ… ì»´íŒŒì¼ ìŠ¤í¬ë¦½íŠ¸ ì¤€ë¹„ ì™„ë£Œ
3. â³ FuriosaAI NPU í•˜ë“œì›¨ì–´ì—ì„œ ì‹¤ì œ ì»´íŒŒì¼
4. â³ ì¶”ë¡  ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
5. â³ í”„ë¡œë•ì…˜ ë°°í¬

---

**ìƒì„±ì¼**: 2025-11-11
**SDK ë²„ì „**: FuriosaAI SDK 2025.3.1
**ëª¨ë¸**: Llama-3.1-8B-Instruct
