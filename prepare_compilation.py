#!/usr/bin/env python3
"""
FuriosaAI NPUë¥¼ ìœ„í•œ Llama ëª¨ë¸ ì»´íŒŒì¼ ì¤€ë¹„ ìŠ¤í¬ë¦½íŠ¸
"""
import os
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# ì„¤ì •
MODEL_DIR = Path("./models/Llama-3.1-8B-Instruct")
OUTPUT_DIR = Path("./compiled_models")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

print("ğŸ”§ FuriosaAI Llama Model Compilation Setup")
print("=" * 60)

# 1. ëª¨ë¸ ì •ë³´ í™•ì¸
print(f"\nğŸ“‚ Model directory: {MODEL_DIR.absolute()}")

if not MODEL_DIR.exists():
    print(f"âŒ Model directory not found: {MODEL_DIR}")
    print("   Please run download_model.py first!")
    exit(1)

# 2. í† í¬ë‚˜ì´ì € ë¡œë“œ
print("\nğŸ“ Loading tokenizer...")
try:
    tokenizer = AutoTokenizer.from_pretrained(str(MODEL_DIR))
    print(f"âœ… Tokenizer loaded successfully")
    print(f"   Vocabulary size: {tokenizer.vocab_size}")
except Exception as e:
    print(f"âŒ Error loading tokenizer: {e}")
    raise

# 3. ëª¨ë¸ êµ¬ì„± í™•ì¸
print("\nğŸ” Checking model configuration...")
try:
    from transformers import AutoConfig
    config = AutoConfig.from_pretrained(str(MODEL_DIR))

    print(f"âœ… Model configuration:")
    print(f"   Architecture: {config.architectures[0]}")
    print(f"   Hidden size: {config.hidden_size}")
    print(f"   Num layers: {config.num_hidden_layers}")
    print(f"   Num attention heads: {config.num_attention_heads}")
    print(f"   Vocab size: {config.vocab_size}")
    print(f"   Max position embeddings: {config.max_position_embeddings}")

except Exception as e:
    print(f"âŒ Error loading config: {e}")
    raise

# 4. ì»´íŒŒì¼ ì •ë³´
print("\nğŸ“‹ Next Steps for FuriosaAI Compilation:")
print("   1. Install furiosa-llm package (if available)")
print("      $ pip install furiosa-llm")
print("   2. Use FuriosaAI's LLM compilation tools")
print("   3. Or use optimum-furiosa for integration")
print()
print("ğŸ“ Example compilation command structure:")
print("   furiosa-llm compile \\")
print(f"     --model-path {MODEL_DIR} \\")
print(f"     --output-path {OUTPUT_DIR} \\")
print("     --batch-size 1 \\")
print("     --seq-length 2048")
print()

# 5. í…ŒìŠ¤íŠ¸ ì…ë ¥ ìƒì„±
print("\nğŸ§ª Generating test input...")
test_text = "Hello, I am a language model"
inputs = tokenizer(test_text, return_tensors="pt")
print(f"âœ… Test input generated:")
print(f"   Text: '{test_text}'")
print(f"   Input IDs shape: {inputs['input_ids'].shape}")
print(f"   Attention mask shape: {inputs['attention_mask'].shape}")

print("\nâœ… Compilation preparation complete!")
print(f"ğŸ“ Model ready at: {MODEL_DIR.absolute()}")
print(f"ğŸ“ Output directory: {OUTPUT_DIR.absolute()}")
